{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Markdown cheatsheet](https://daringfireball.net/projects/markdown/syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analysis of Discourse Macrostructure in Schizophrenia: a Corpus Study</h1>\n",
    "<b><i>Ryazanskaya Galina,\n",
    "    Center for Language and Brain, HSE</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>my plans</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[my secrert plan for this project and more](https://docs.google.com/document/d/1GATeTSyHrhbg5M_5hWci77V3ys_P0gS8KzdyQY1JNLE/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus of Russian internet language I am using is called RuWac. Here I have a link: \n",
    "[RuWac about](https://bitbucket.org/kostialopuhin/russe)\n",
    "\n",
    "I also provide a link for direct download of the corpus archive (9GB): \n",
    "[RuWac](http://corpus.leeds.ac.uk/tools/ru/ruwac-parsed.out.xz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>let us install everything</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__let us also download the archived corpus__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"http://corpus.leeds.ac.uk/tools/ru/ruwac-parsed.out.xz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>let us now do the things we will repeat many times as we work</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git commit modeling_schizo.ipynb -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import lzma\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from progressbar import progressbar\n",
    "from string import punctuation as punct\n",
    "from string import ascii_letters as latin\n",
    "punct += '»«'\n",
    "\n",
    "corpus_archive_filename = 'ruwac-parsed.out.xz'\n",
    "corpus_filename = 'corpus.txt'\n",
    "premodel_filename = 'small.model'\n",
    "vocabulary_filename = 'freq.txt'\n",
    "model_filename = 'full.model'\n",
    "sample_directory = 'sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>let us take the corpus out of the archive</h2>\n",
    "\n",
    "__let us also take lemmas and make out of them a file with \\n-separated sentences, including only sentences not containing latin symbols__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lzma.open(corpus_archive_filename, mode='rt', encoding='utf-8') as f:\n",
    "    with open(corpus_filename, mode='w', encoding='utf-8') as t:\n",
    "        current = []\n",
    "        for line_number, line in enumerate(f):\n",
    "            if line_number % 10000000 == 0:\n",
    "                print(line_number, datetime.now().time(), file=open('archive_log.txt', 'a', encoding='utf-8'))\n",
    "            l = line.split('\\t')\n",
    "            if len(l) == 7:\n",
    "                lemm = l[3]\n",
    "                sent = l[1]\n",
    "                if sent == 'SENT':\n",
    "                    if set(latin).intersection(set(''.join(current))) == set():\n",
    "                        current.append('\\n')\n",
    "                        print(' '.join(current), file=t, end='')\n",
    "                    current = []\n",
    "                elif lemm not in punct:\n",
    "                    current.append(lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__here are some examples for me__\n",
    "\n",
    "[word2vec source link](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "[word2vec tutorial](https://rare-technologies.com/word2vec-tutorial/)\n",
    "\n",
    "[my previous work on w2v](https://github.com/flying-bear/HSE_programming/blob/master/HSE_programming_2/hws/hw_word2vec/HW_word2vec.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>let us create a subset of corpus for testing before training on the big corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(corpus_filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                if len(sentences) < 1000:\n",
    "                    sentences.append(line.split())\n",
    "                else:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lgpc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "2018-12-01 20:08:20,475 : INFO : collecting all words and their counts\n",
      "2018-12-01 20:08:20,483 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-01 20:08:20,499 : INFO : collected 3542 word types from a corpus of 12137 raw words and 1000 sentences\n",
      "2018-12-01 20:08:20,503 : INFO : Loading a fresh vocabulary\n",
      "2018-12-01 20:08:20,539 : INFO : effective_min_count=1 retains 3542 unique words (100% of original 3542, drops 0)\n",
      "2018-12-01 20:08:20,539 : INFO : effective_min_count=1 leaves 12137 word corpus (100% of original 12137, drops 0)\n",
      "2018-12-01 20:08:20,591 : INFO : deleting the raw counts dictionary of 3542 items\n",
      "2018-12-01 20:08:20,599 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2018-12-01 20:08:20,603 : INFO : downsampling leaves estimated 10134 word corpus (83.5% of prior 12137)\n",
      "2018-12-01 20:08:20,635 : INFO : estimated required memory for 3542 words and 100 dimensions: 4604600 bytes\n",
      "2018-12-01 20:08:20,647 : INFO : resetting layer weights\n",
      "2018-12-01 20:08:20,873 : INFO : training model with 3 workers on 3542 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-12-01 20:08:20,963 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:08:24,814 : INFO : EPOCH 1 - PROGRESS: at 19.40% examples, 455 words/s, in_qsize 1, out_qsize 1\n",
      "2018-12-01 20:08:24,814 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:08:30,851 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 1020 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:08:30,851 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:08:30,870 : INFO : EPOCH - 1 : training on 12137 raw words (10152 effective words) took 10.0s, 1019 effective words/s\n",
      "2018-12-01 20:08:30,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:08:35,213 : INFO : EPOCH 2 - PROGRESS: at 19.40% examples, 413 words/s, in_qsize 1, out_qsize 1\n",
      "2018-12-01 20:08:35,228 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:08:40,903 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 1010 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:08:40,903 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:08:40,919 : INFO : EPOCH - 2 : training on 12137 raw words (10109 effective words) took 10.0s, 1008 effective words/s\n",
      "2018-12-01 20:08:40,957 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:08:44,664 : INFO : EPOCH 3 - PROGRESS: at 19.40% examples, 483 words/s, in_qsize 1, out_qsize 1\n",
      "2018-12-01 20:08:44,680 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:08:49,992 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 1116 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:08:49,992 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:08:50,008 : INFO : EPOCH - 3 : training on 12137 raw words (10113 effective words) took 9.1s, 1114 effective words/s\n",
      "2018-12-01 20:08:50,056 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:08:53,907 : INFO : EPOCH 4 - PROGRESS: at 19.40% examples, 470 words/s, in_qsize 1, out_qsize 1\n",
      "2018-12-01 20:08:53,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:08:59,086 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 1120 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:08:59,101 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:08:59,101 : INFO : EPOCH - 4 : training on 12137 raw words (10144 effective words) took 9.1s, 1118 effective words/s\n",
      "2018-12-01 20:08:59,157 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:09:02,858 : INFO : EPOCH 5 - PROGRESS: at 19.40% examples, 484 words/s, in_qsize 1, out_qsize 1\n",
      "2018-12-01 20:09:02,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:09:08,204 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 1119 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:09:08,204 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:09:08,204 : INFO : EPOCH - 5 : training on 12137 raw words (10145 effective words) took 9.1s, 1118 effective words/s\n",
      "2018-12-01 20:09:08,242 : INFO : training on a 60685 raw words (50663 effective words) took 47.4s, 1070 effective words/s\n",
      "2018-12-01 20:09:08,258 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "premodel = gensim.models.Word2Vec(sentences, min_count = 1, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__let us check and save the test model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00033608,  0.01961686, -0.0060016 ,  0.02352651,  0.0161804 ,\n",
       "        0.00198638, -0.0156463 , -0.00617301, -0.0008622 ,  0.00333268,\n",
       "       -0.01050657, -0.00588358, -0.00142256,  0.0070704 ,  0.0032397 ,\n",
       "       -0.0143547 ,  0.00678462, -0.00413314,  0.00284421, -0.00290856,\n",
       "        0.00344219, -0.01387355,  0.00470964, -0.01163711,  0.00467778,\n",
       "       -0.00308427,  0.01401065, -0.00525512, -0.00533063, -0.00529045,\n",
       "        0.00236804,  0.00438249,  0.00122866, -0.0019901 ,  0.01409124,\n",
       "        0.00199817,  0.01031996,  0.00540237, -0.01548326, -0.00977472,\n",
       "        0.00738592, -0.00249218,  0.00210615,  0.0234709 , -0.00541235,\n",
       "        0.00151491,  0.00526316, -0.00303754, -0.0066194 , -0.0012779 ,\n",
       "        0.00284218, -0.01179871, -0.0174361 ,  0.00898844,  0.00195034,\n",
       "        0.01008916, -0.01302091, -0.00645701,  0.0221268 , -0.00095577,\n",
       "       -0.01296543, -0.01397609, -0.00992239, -0.00804624, -0.00208522,\n",
       "        0.00354438, -0.0281995 ,  0.01097893, -0.00234392, -0.00284742,\n",
       "       -0.00063001, -0.00365237,  0.02263813,  0.00605768, -0.01006803,\n",
       "       -0.00294228, -0.00412407,  0.0035295 , -0.00889006,  0.00342549,\n",
       "        0.01315972, -0.0007021 , -0.00109579,  0.01178587, -0.00236727,\n",
       "        0.00275599,  0.00569879,  0.00657988, -0.00898824, -0.00378519,\n",
       "       -0.01754965,  0.0264187 ,  0.00513498, -0.01064842, -0.01356151,\n",
       "       -0.00714472, -0.00305994, -0.0004094 ,  0.008535  ,  0.00726308],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premodel.wv['я']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('это', <gensim.models.keyedvectors.Vocab at 0xf4627b5358>),\n",
       " ('тип', <gensim.models.keyedvectors.Vocab at 0xf4627b53c8>),\n",
       " ('девушка', <gensim.models.keyedvectors.Vocab at 0xf4627b5588>),\n",
       " ('в', <gensim.models.keyedvectors.Vocab at 0xf4627b55c0>),\n",
       " ('один', <gensim.models.keyedvectors.Vocab at 0xf4627b56a0>),\n",
       " ('день', <gensim.models.keyedvectors.Vocab at 0xf4627b56d8>),\n",
       " ('отмечать', <gensim.models.keyedvectors.Vocab at 0xf4627b5710>),\n",
       " ('что', <gensim.models.keyedvectors.Vocab at 0xf4627b5748>),\n",
       " ('на', <gensim.models.keyedvectors.Vocab at 0xf4627b5780>),\n",
       " ('себя', <gensim.models.keyedvectors.Vocab at 0xf4627b57b8>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(premodel.wv.vocab.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lgpc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\lgpc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.036809895"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premodel.similarity('найти', 'второй')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lgpc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('тотальнее', 0.35982415080070496),\n",
       " ('управляемый', 0.3162972331047058),\n",
       " ('решить', 0.3074740767478943),\n",
       " ('живой', 0.29716986417770386),\n",
       " ('баррикада', 0.29420873522758484),\n",
       " ('владелец', 0.29158836603164673),\n",
       " ('верить', 0.29105377197265625),\n",
       " ('маскировка', 0.29051047563552856),\n",
       " ('разгар', 0.28887930512428284),\n",
       " ('возможно', 0.28510481119155884)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premodel.most_similar(positive=['банк'], negative=[], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__let us save it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:11:08,616 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2018-12-01 20:11:08,632 : INFO : not storing attribute vectors_norm\n",
      "2018-12-01 20:11:08,640 : INFO : not storing attribute cum_table\n",
      "2018-12-01 20:11:08,824 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "premodel.save(premodel_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__now we can load it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-10 22:16:39,757 : INFO : loading Word2Vec object from small.model\n",
      "2018-12-10 22:16:39,884 : INFO : loading wv recursively from small.model.wv.* with mmap=None\n",
      "2018-12-10 22:16:39,894 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-12-10 22:16:39,903 : INFO : loading vocabulary recursively from small.model.vocabulary.* with mmap=None\n",
      "2018-12-10 22:16:39,909 : INFO : loading trainables recursively from small.model.trainables.* with mmap=None\n",
      "2018-12-10 22:16:39,915 : INFO : setting ignored attribute cum_table to None\n",
      "2018-12-10 22:16:39,920 : INFO : loaded small.model\n"
     ]
    }
   ],
   "source": [
    "premodel = gensim.models.Word2Vec.load(premodel_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>let us run the training on the full corpust text now</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>let us generate a vocabulary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            for word in line.split():\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                    #                     | 1296566294 Elapsed Time: 0:48:07\n"
     ]
    }
   ],
   "source": [
    "word_freq = Counter(progressbar(get_vocabulary(corpus_filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__let us save it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocabulary_filename, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__now we can just read the vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocabulary_filename, 'r', encoding='utf-8') as file:\n",
    "    word_freq = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>let us give the model the vocabulary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sg=1, min_count=10, workers=1, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='vocabulary.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab_from_freq(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__let us save it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>let us train the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='training.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100046103, 1296566295)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(get_sentences(corpus_filename), epochs=1, total_examples=len(word_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__let us save it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gryazanskaya/modeling_schizo/env/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/gryazanskaya/modeling_schizo/env/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('сбербанк', 0.8356039524078369),\n",
       " ('банковский', 0.8246872425079346),\n",
       " ('кредитный', 0.8222591876983643),\n",
       " ('ипотечный', 0.8164801597595215),\n",
       " ('банка', 0.804668664932251),\n",
       " ('втб', 0.7961848974227905),\n",
       " ('сберегательный', 0.7937512397766113),\n",
       " ('цб', 0.790773332118988),\n",
       " ('кредит', 0.7881439924240112),\n",
       " ('эмитент', 0.7846423387527466)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['банк'], negative=[], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__now we can load it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-10 22:21:20,385 : INFO : loading Word2Vec object from full.model\n",
      "2018-12-10 22:21:27,576 : INFO : loading trainables recursively from full.model.trainables.* with mmap=None\n",
      "2018-12-10 22:21:27,579 : INFO : loading syn1neg from full.model.trainables.syn1neg.npy with mmap=None\n",
      "2018-12-10 22:21:36,578 : INFO : loading wv recursively from full.model.wv.* with mmap=None\n",
      "2018-12-10 22:21:36,585 : INFO : loading vectors from full.model.wv.vectors.npy with mmap=None\n",
      "2018-12-10 22:21:42,983 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-12-10 22:21:43,114 : INFO : loading vocabulary recursively from full.model.vocabulary.* with mmap=None\n",
      "2018-12-10 22:21:43,120 : INFO : setting ignored attribute cum_table to None\n",
      "2018-12-10 22:21:43,145 : INFO : loaded full.model\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>let us prepare the texts from the sample</h2>\n",
    "\n",
    "__the sample consists of__ \n",
    "\n",
    "+ 9 outpatients diagnosed with schizophrenia\n",
    "+ 10 controls from Russian Clinical Pear Stories Corpus (CliPS)\n",
    "\n",
    "the original CliPS corpus contains pear film retellings from people with aphasia, traumatic brain injuries, as well as healthy controls\n",
    "\n",
    "the files contain already lemmatized texts with \\n separated sentences (discourse units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {'control': {}, 'schizo': {}}\n",
    "\n",
    "for filename in os.listdir(sample_directory):\n",
    "    file_path = sample_directory + '/' + filename\n",
    "    name = filename.strip('.txt')\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_sent = file.readlines()\n",
    "    sent = []\n",
    "    for raw in raw_sent:\n",
    "        sent.append(raw.strip().split())\n",
    "    if 'H' in filename:\n",
    "        sample['control'][name] = sent\n",
    "    elif 'S' in filename:\n",
    "        sample['schizo'][name] = sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us ensure that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['вот', 'нет', 'текст'],\n",
       " ['это', 'так', 'задумать', 'да'],\n",
       " ['ну', 'я', 'думать', 'что', 'это', 'такой', 'такой', 'сюжетик', 'фильм'],\n",
       " ['значит',\n",
       "  'это',\n",
       "  'где-то',\n",
       "  'южный',\n",
       "  'страна',\n",
       "  'может',\n",
       "  'Мексика',\n",
       "  'может',\n",
       "  'что'],\n",
       " ['человек', 'там'],\n",
       " ['дело',\n",
       "  'в',\n",
       "  'то',\n",
       "  'что',\n",
       "  'это',\n",
       "  'не',\n",
       "  'на',\n",
       "  'свой',\n",
       "  'участок',\n",
       "  'видно',\n",
       "  'человек',\n",
       "  'собирать',\n",
       "  'урожай',\n",
       "  'груша'],\n",
       " ['большой',\n",
       "  'груша',\n",
       "  'стоять',\n",
       "  'дикий',\n",
       "  'ну',\n",
       "  'большой',\n",
       "  'такой',\n",
       "  'полноценный',\n",
       "  'груша',\n",
       "  'не',\n",
       "  'дичка',\n",
       "  'какой-то',\n",
       "  'там',\n",
       "  'вот'],\n",
       " ['и', 'в', 'общем', 'должен', 'быть', 'три', 'корзина', 'нагрузить', 'вот'],\n",
       " ['ну',\n",
       "  'в',\n",
       "  'один',\n",
       "  'момент',\n",
       "  'он',\n",
       "  'значит',\n",
       "  'сделать',\n",
       "  'с',\n",
       "  'один',\n",
       "  'груша',\n",
       "  'видно',\n",
       "  'как',\n",
       "  'благодарность',\n",
       "  'этот',\n",
       "  'груша',\n",
       "  'что',\n",
       "  'они',\n",
       "  'он',\n",
       "  'может',\n",
       "  'бесплатно',\n",
       "  'достваться',\n",
       "  'он',\n",
       "  'сделать',\n",
       "  'ритуал'],\n",
       " ['вытереть', 'один', 'груша', 'свой', 'шейный', 'платок', 'вот'],\n",
       " ['потом', 'подъехать', 'мальчик'],\n",
       " ['но',\n",
       "  'мальчик',\n",
       "  'видно',\n",
       "  'я',\n",
       "  'думать',\n",
       "  'что',\n",
       "  'это',\n",
       "  'немножко',\n",
       "  'тут',\n",
       "  'разыграть'],\n",
       " ['это', 'его', 'или', 'сын', 'младший', 'сынок', 'его', 'или', 'сосед'],\n",
       " ['но',\n",
       "  'я',\n",
       "  'думать',\n",
       "  'что',\n",
       "  'младший',\n",
       "  'сын',\n",
       "  'должен',\n",
       "  'быть',\n",
       "  'приехать',\n",
       "  'вот',\n",
       "  'за',\n",
       "  'груша'],\n",
       " ['ну', 'он', 'приехать', 'может', 'рано', 'или', 'как-то'],\n",
       " ['а',\n",
       "  'отец',\n",
       "  'настолько',\n",
       "  'быть',\n",
       "  'увлеченный',\n",
       "  'сбор',\n",
       "  'груш',\n",
       "  'что',\n",
       "  'он',\n",
       "  'раз',\n",
       "  'погрузить',\n",
       "  'и',\n",
       "  'уехать',\n",
       "  'тихонечко'],\n",
       " ['по', 'дорога', 'он', 'встретить', 'девочка', 'который', 'он', 'нравиться'],\n",
       " ['он', 'увлечься'],\n",
       " ['и', 'наехать', 'на', 'камень'],\n",
       " ['рассыпаться', 'груша'],\n",
       " ['и',\n",
       "  'тут',\n",
       "  'выйти',\n",
       "  'друзья',\n",
       "  'его',\n",
       "  'по',\n",
       "  'этот',\n",
       "  'поселок',\n",
       "  'который',\n",
       "  'не',\n",
       "  'агрессивный',\n",
       "  'а',\n",
       "  'наоборот',\n",
       "  'помочь',\n",
       "  'он',\n",
       "  'собрать',\n",
       "  'все',\n",
       "  'вот'],\n",
       " ['и', 'даже', 'потом', 'он', 'уронить', 'шляпа'],\n",
       " ['он', 'тоже', 'дружок', 'поднять', 'он', 'шляпа'],\n",
       " ['а',\n",
       "  'в',\n",
       "  'знак',\n",
       "  'благодарность',\n",
       "  'что',\n",
       "  'он',\n",
       "  'он',\n",
       "  'несколько',\n",
       "  'груша',\n",
       "  'дать',\n",
       "  'вот',\n",
       "  'отблагодарить',\n",
       "  'вот'],\n",
       " ['ну', 'удариться', 'он', 'при', 'падение', 'колено'],\n",
       " ['у', 'тебя'],\n",
       " ['рассмотреть', 'травма', 'вот'],\n",
       " ['ну',\n",
       "  'а',\n",
       "  'отец',\n",
       "  'когда',\n",
       "  'спуститься',\n",
       "  'в',\n",
       "  'недоумение',\n",
       "  'немножко',\n",
       "  'понять',\n",
       "  'что',\n",
       "  'куда',\n",
       "  'корзина-то',\n",
       "  'деться',\n",
       "  'одна'],\n",
       " ['вроде', 'три', 'быть', 'а', 'одна', 'корзина', 'нет'],\n",
       " ['ну', 'вот', 'и', 'тут', 'мальчик', 'этот', 'идти'],\n",
       " ['и',\n",
       "  'он',\n",
       "  'не',\n",
       "  'понять',\n",
       "  'у',\n",
       "  'они',\n",
       "  'нет',\n",
       "  'при',\n",
       "  'себя',\n",
       "  'ни',\n",
       "  'корзина'],\n",
       " ['ну',\n",
       "  'у',\n",
       "  'кто-то',\n",
       "  'кто-то',\n",
       "  'там',\n",
       "  'груша',\n",
       "  'доедать',\n",
       "  'или',\n",
       "  'еще',\n",
       "  'нести'],\n",
       " ['недоумение', 'как', 'бы', 'такой', 'вот'],\n",
       " ['такой', 'маленький', 'комедия'],\n",
       " ['или', 'как', 'бы', 'сказать', 'сюжет', 'такой']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['control']['HP-03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>let us finally apply the measures</h2>\n",
    "\n",
    "- __Tangentiality__ (see [Elvevåg et al 2017](https://academic.oup.com/schizophreniabulletin/article/43/3/509/3574458))\n",
    "<ul>\n",
    "    <li>several graphs for varying spans (recomended minimum meaningful span ~8): <br>\n",
    "        x axis: distance between the spans in words <br>\n",
    "        y axis: cosine distance between an averaged vector of the first span and the current span\n",
    "    </li>\n",
    "    <li>a graph: <br>\n",
    "        x axis: distance between the clauses (measured in clauses or words) <br>\n",
    "        y axis: cosine distance between an averaged vector of the first clause and the current clause\n",
    "    </li>\n",
    "</ul>\n",
    "- __Local coherence__ <br>\n",
    "    (let i = cosine distance between an averaged vector of the neighbouring clauses)\n",
    "<ul>\n",
    "    <li>average i, range of i, SD of i for each participant</li>\n",
    "    <li>the differences between the averages for the groups</li>\n",
    "    <li>the differences between the averages for the averages of the groups</li>\n",
    "    <li>a graph: <br>\n",
    "        x axis: number of the current clause <br>\n",
    "        y axis: i</li>\n",
    "</ul>\n",
    "- __Global coherence__ <br>\n",
    "    (let us take a centroid of each clause, then a centroid of these centroids in one text, then a centroid of the centroids of all the texts at hand)\n",
    "<ul>\n",
    "    <li>cosine distance between the centroid of all the texsts and the centroid of the current text for each partcipant</li>\n",
    "    <li>cosine distance between the centroid of all the <i>control</i> texsts and the centroid of the current text for each partcipant</li>\n",
    "</ul>\n",
    "- __Violations of completeness__ (see levenshtein, needleman, wunsch [wiki: Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance) and [wiki: Needleman–Wunsch_algorithm](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm))<br>\n",
    "    (let the set of topics be defined as text (set_of_propositions.txt))\n",
    "<ul>\n",
    "    <li>cosine distance between the centroid of the topics and the centroid of the text</li>\n",
    "    <li>cosine distance between each topic and each clause and alignment levenshtein, needleman, wunsch (pay attention to the weight of the alignment)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
